I"’<p>In the previous <a href="https://dataisblue.io/python,/data/science/2020/02/15/random-forest-is-not-calibrated.html">blog post</a> , we looked at the probability predictions that come out of naive implementation of the <code class="highlighter-rouge">scikit-learn</code> Random Forest classifier. We noted that the predictions are not well-calibrated, but did not address how to fix that problem, which is the subject of this blog post.</p>

<!--more-->

<h2 id="calibrating-a-random-forest-classifier">Calibrating a Random Forest Classifier</h2>
<p>It turns out that calibrating the random forest classifier in <code class="highlighter-rouge">scikit-learn</code> is really only a matter of an additional line or two of code, as <code class="highlighter-rouge">scikit-learn</code> helpfully comes with a method to do exactly the thing we want: the <a href="https://scikit-learn.org/stable/modules/generated/sklearn.calibration.CalibratedClassifierCV.html#sklearn.calibration.CalibratedClassifierCV">CalibratedClassifierCV</a> class. This class uses a train set to fit the original method, and then uses a test set to calibrate the probabilities afterwards. Note that it works for any classifier method in the scikit package, not just for random forest.</p>

<p>There are two methods for calibration available in this class - isotonic and sigmoid. The sigmoid method corresponds to something called <strong>Plattâ€™s Method</strong> or <strong>Plattâ€™s scaling</strong>. This method essentially fits a logistic regression to the output of the original classifier. According to the <a href="https://en.wikipedia.org/wiki/Platt_scaling">Wikipedia article</a></p>

<blockquote>
  <p>Platt scaling has been shown to be effective for SVMs as well as other types of classification models, including boosted models and even naive Bayes classifiers, which produce distorted probability distributions. It is particularly effective for max-margin methods such as SVMs and boosted trees, which show sigmoidal distortions in their predicted probabilities, but has less of an effect with well-calibrated models such as logistic regression, multilayer perceptrons, and random forests.</p>
</blockquote>

<p>I was interested to note that this particular Wikipedia article considers random forest models to be well-calibrated! (I donâ€™t.)</p>

<p>What weâ€™ll be using in this example, though, is the <a href="https://en.wikipedia.org/wiki/Isotonic_regression">isotonic</a> method, which is a non-parametric method that works well if there is sufficient data in the test set.</p>

<h2 id="show-me">Show me</h2>

<p>The notebook with the full code is available <a href="https://github.com/PhysB/blog_notebooks/blob/master/Calibrating%20a%20Random%20Forest.ipynb">here</a>.</p>

<h3 id="create-classification-data">Create classification data</h3>

<p>As before, weâ€™ll start by creating some classification data.</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>X, y = make_classification(n_samples=20000, n_features=3, n_informative=3, n_redundant=0, random_state=0, shuffle=False)
</code></pre></div></div>

<p><img src="/assets/classification2.png" alt="Classification data" class="align-center" /></p>

<h3 id="train-a-random-forest-both-with-and-without-calibratedclassifiercv">Train a random forest both with and without CalibratedClassifierCV.</h3>

<p>Split the data into train and test sets:</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)
</code></pre></div></div>

<p>Random forest trained with no calibration:</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>clf = RandomForestClassifier(max_depth=3, random_state=0, min_samples_leaf=3).fit(X_train, y_train)
predictions_rf = clf.predict_proba(X_test)
</code></pre></div></div>

<p>Random forest trained with CalibratedClassifierCV:</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>clf = RandomForestClassifier(max_depth=3, random_state=0, min_samples_leaf=3)
clf_isotonic = CalibratedClassifierCV(clf, method='isotonic')
clf_isotonic.fit(X_train, y_train)
predictions_isotonic = clf_isotonic.predict_proba(X_test)
</code></pre></div></div>

<h3 id="bin-the-predictions-and-plot-the-two-sets-of-predictions">Bin the predictions and plot the two sets of predictions</h3>
<p>We can see that the curve corresponding to the isotonic calibration now tracks much closer to the <code class="highlighter-rouge">x=y</code> line (which would indicate perfect calibration). Itâ€™s not perfect, but itâ€™s much better than probability predictions coming out of the uncalibrated random forest classifier. Note especially how much better it does at the high and low ends of the calibration curve.</p>

<p><img src="/assets/random_forest_isotonic.png" alt="Classification data" class="align-center" /></p>

<p>We can also get a numerical estimate of how much better weâ€™re doing by using the <a href="https://en.wikipedia.org/wiki/Brier_score">Brierâ€™s Score</a>, which is really just the mean squared error of the predicted probabilities.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>print("Brier scores: (smaller is better)")

clf_score = brier_score_loss(y_test, predictions_rf[:,1])
print("No calibration: %1.3f" % clf_score)

clf_isotonic_score = brier_score_loss(y_test, predictions_isotonic[:,1])
print("With isotonic calibration: %1.3f" % clf_isotonic_score)
</code></pre></div></div>

<p>The output of this score for these two models is</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Brier scores: (smaller is better)
No calibration: 0.152
With isotonic calibration: 0.094
</code></pre></div></div>

<p>Which confirms what we can also see by eye in the plot aboveâ€“the calibrated random forest produces better probability estimates.</p>

<h2 id="conclusion">Conclusion</h2>
<p>In this two-part blog post, weâ€™ve seen that, on its own, the random forest classifier poorly calibrated. Luckily, itâ€™s only a matter of another two lines of code to get the well-calibrated random forest classifier of your dreams.</p>

:ET